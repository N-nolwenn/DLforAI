{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0JRvO8w8EN-"
      },
      "source": [
        "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
        "\n",
        "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
        "\n",
        "This notebook was generated for TensorFlow 2.6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RB5y6hzr8EOD"
      },
      "source": [
        "### Processing words as a sequence: The sequence model approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emE_4YRf8EOD"
      },
      "source": [
        "#### A first practical example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOg5v5Jm8EOE"
      },
      "source": [
        "**Downloading the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUyO4ikb8EOE",
        "outputId": "a40100c6-f269-4f50-db25-9eb33e5ee984"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  58.0M      0  0:00:01  0:00:01 --:--:-- 58.0M\n"
          ]
        }
      ],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz\n",
        "!rm -r aclImdb/train/unsup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjKRcOSf8EOF"
      },
      "source": [
        "**Preparing the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRx4ATWQ8EOG",
        "outputId": "82cd948f-cd19-4d2d-c127-d511b125afbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "import os, pathlib, shutil, random\n",
        "from tensorflow import keras\n",
        "batch_size = 32\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(val_dir / category)\n",
        "    files = os.listdir(train_dir / category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    num_val_samples = int(0.2 * len(files))\n",
        "    val_files = files[-num_val_samples:]\n",
        "    for fname in val_files:\n",
        "        shutil.move(train_dir / category / fname,\n",
        "                    val_dir / category / fname)\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmXIySZ_8EOH"
      },
      "source": [
        "**Preparing integer sequence datasets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AiklMnCc8EOI"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "max_length = 600\n",
        "max_tokens = 20000\n",
        "text_vectorization = layers.TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    output_mode=\"int\", #sequential information is preserved\n",
        "    output_sequence_length=max_length,\n",
        "#In order to keep a manageable input size we ll truncate the inputs after the first 600 words.\n",
        "#This is a reasonable choice, since the average review length is 233 words and only 5% of reviews are longer than 600 words\n",
        ")\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "#process train, validation and test data sets\n",
        "int_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "int_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "int_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGV1crj38EOJ"
      },
      "source": [
        "**A sequence model built on one-hot encoded vector sequences**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The simplest way to convert our integer sequences to vector sequences is to use one hot encoding. On top of these one-hot vecotrs we'll add a simple bidirectional LSTM "
      ],
      "metadata": {
        "id": "BVmzQMpRVGgW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7dbuQtL8EOJ",
        "outputId": "aaf11954-1d6e-4e10-e815-f5ff4a80a324"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " tf.one_hot (TFOpLambda)     (None, None, 20000)       0         \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 64)               5128448   \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,128,513\n",
            "Trainable params: 5,128,513\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\") #One input is a sequence of integers\n",
        "embedded = tf.one_hot(inputs, depth=max_tokens) #encode the integers into binary 20000 dimensional vectors\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded) #add a bidirectional LSTM\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)#Finally add a classification layer\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3s4Bdeh8EOJ"
      },
      "source": [
        "**Training a first basic sequence model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Kd8SshH8EOK",
        "outputId": "23eb71fd-1079-4572-9d90-45fb22e3d72b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 172s 261ms/step - loss: 0.5452 - accuracy: 0.7362 - val_loss: 0.4607 - val_accuracy: 0.7832\n",
            "Epoch 2/10\n",
            " 73/625 [==>...........................] - ETA: 2:15 - loss: 0.4093 - accuracy: 0.8446"
          ]
        }
      ],
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"one_hot_bidir_lstm.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "model = keras.models.load_model(\"one_hot_bidir_lstm.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdzO1n0O8EOK"
      },
      "source": [
        "#### Understanding word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another popuar and powerful way to associate a vector with a word is the use of dense word vectos also called word embeddings\n",
        "\n",
        "The geometric relationship betw/ 2 word vectors should reflect the semantic relationchip between these words."
      ],
      "metadata": {
        "id": "myE_QCv8Vvp4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two Ways to obtain word embeddings:\n",
        "Learn word embeddings jointy with the main task you care about (such as document classification or sentimentprediction)\n",
        "\n",
        "Load into your model word embeddings that were precomputed using a different machine learning task than the one you're trying to solve. \n",
        "\n",
        "These are called pretrained word embeddings"
      ],
      "metadata": {
        "id": "nX2B-zkjXARN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z80ZvDnT8EOK"
      },
      "source": [
        "#### Learning word embeddings with the Embedding layer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simplest way to associate a dense vector with a word is to choose the vector at random.\n",
        "\n",
        "To get a bit more abstract the geometric relationships betw/ word vectors should reflect the semantic relationchips bet/ thses words."
      ],
      "metadata": {
        "id": "jPPR14CnXgvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To learn a new embedding space with every new task"
      ],
      "metadata": {
        "id": "46zinqrsXyPF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9NNCehr8EOL"
      },
      "source": [
        "**Instantiating an `Embedding` layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvY9XO8E8EOL"
      },
      "outputs": [],
      "source": [
        "embedding_layer = layers.Embedding(input_dim=max_tokens, output_dim=256)\n",
        "#The Embedding layer takes at least 2 arguments: the number of possible tokens and the dimensionality of the embeddings (here, 256)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Embedding layer is best understood as a dictionary that maps integer indices (which stand for specific words) to dense vectors.\n",
        "\n",
        "Word index -> Embedding layer -> Corresponding word vector"
      ],
      "metadata": {
        "id": "vfTamE03X2dQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rc_NXZcp8EOL"
      },
      "source": [
        "**Model that uses an `Embedding` layer trained from scratch**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RG0pt2-48EOL"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "model = keras.models.load_model(\"embeddings_bidir_gru.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wt71va528EOM"
      },
      "source": [
        "#### Understanding padding and masking"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Padding: Sentences longer than K tokens are truncated to a lenght of K tokens, and sentences shorter than K tokens are padded with zeros at the end so that they can be concatenated together with other sequences.\n",
        "\n",
        "We may have too many zeros for shorter sequences. The information stored in the internal state of the RNN will gradually fade out as it gets exposed to these meaningless inputs.\n",
        "\n",
        "Masking: Tell the RNN that is should skip these iterations of zeros"
      ],
      "metadata": {
        "id": "z1Y21Q1jY-fb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFGfKtYV8EOM"
      },
      "source": [
        "**Using an `Embedding` layer with masking enabled**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YIw4pjh8EOM"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = layers.Embedding(\n",
        "    input_dim=max_tokens, output_dim=256, mask_zero=True)(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru_with_masking.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "model = keras.models.load_model(\"embeddings_bidir_gru_with_masking.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eetk0qo8EOM"
      },
      "source": [
        "#### Using pretrained word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Somestimes word embeddings have so little training data available that you can't use your data alone to learn an appropriate task specific embedding of your vocabulary\n",
        "\n",
        "In such cases instead of learning word embeddings jointly with the problem you want to solve you can load embedding vectors from a precomputed embedding space that you know is highly structured and exhibits useful properties: Word2Vec, GloVe"
      ],
      "metadata": {
        "id": "Y7q-LX8RaK2U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8fuF6jt8EON"
      },
      "outputs": [],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip\n",
        "#to download GloVe word embeddings precomputed on the 2014 English Wikipedia dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwx3ePEP8EON"
      },
      "source": [
        "**Parsing the GloVe word-embeddings file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7JUrQcw8EON"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "path_to_glove_file = \"glove.6B.100d.txt\"\n",
        "\n",
        "embeddings_index = {} #create index list = dictionary \n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs #we save the words vectors\n",
        "\n",
        "print(f\"Found {len(embeddings_index)} word vectors.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wRlmnR58EON"
      },
      "source": [
        "**Preparing the GloVe word-embeddings matrix**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WKIdFyn8EON"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "vocabulary = text_vectorization.get_vocabulary() #1\n",
        "word_index = dict(zip(vocabulary, range(len(vocabulary)))) #2\n",
        "\n",
        "embedding_matrix = np.zeros((max_tokens, embedding_dim)) #3\n",
        "for word, i in word_index.items():\n",
        "    if i < max_tokens:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None: #4\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "#1 Retrieve the vocab indexed by our previous TextVectorization layer\n",
        "#2 Use it to create a mapping from words to their index in the vocabulary\n",
        "#3 Prepare a matrix that we ll fill with the GloVe vectors\n",
        "#4 Fill entry i in the matrix with the word vector for index i. \n",
        "#Words not found in the embedding index will be all zeros"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the pretrained embedding in an Embedding layer"
      ],
      "metadata": {
        "id": "QDO2J4JyH2K5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jgFSck48EON"
      },
      "outputs": [],
      "source": [
        "embedding_layer = layers.Embedding(\n",
        "    max_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        "    mask_zero=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zP3_5UHw8EOO"
      },
      "source": [
        "**Model that uses a pretrained Embedding layer**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So as not to disrupt the pretrained representations during training we freeze the layer via trainable) False"
      ],
      "metadata": {
        "id": "Hepu5dYWH7Hn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xybhw0988EOO"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = embedding_layer(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"glove_embeddings_sequence_model.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "model = keras.models.load_model(\"glove_embeddings_sequence_model.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On this particular task, pretrained embeddings aren't very helpful because the dataset contains enough samples that it s possible to learn a specialized enough embedding space from scratch"
      ],
      "metadata": {
        "id": "5YH6Qs8pIOHl"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "chapter11_part02_sequence-models.i",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}