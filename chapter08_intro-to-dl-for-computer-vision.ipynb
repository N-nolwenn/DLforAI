{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4il-VdHuoB4W"
      },
      "source": [
        "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
        "\n",
        "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
        "\n",
        "This notebook was generated for TensorFlow 2.6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Yc09WRWoB4a"
      },
      "source": [
        "# Introduction to deep learning for computer vision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OEeDZ6voB4b"
      },
      "source": [
        "## Introduction to convnets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stack of Conv2D and MaxPooling2D layers"
      ],
      "metadata": {
        "id": "NgX1lF1IoJ7D"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srA5rI19oB4c"
      },
      "source": [
        "**Instantiating a small convnet**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TyD9kNo_oB4d"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "inputs = keras.Input(shape=(28, 28, 1))\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxN_O3LHoB4f"
      },
      "source": [
        "**Displaying the model's summary**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klmQaOZboB4g",
        "outputId": "9fa690de-0bbe-4406-8269-da33eac869db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 3, 3, 128)         73856     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1152)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                11530     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 104,202\n",
            "Trainable params: 104,202\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruCX90LNoB4h"
      },
      "source": [
        "**Training the convnet on MNIST images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59q48Bk2oB4i",
        "outputId": "a9d50b0c-74a1-4c21-e905-e69256e7e3b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/5\n",
            "938/938 [==============================] - 19s 4ms/step - loss: 0.1559 - accuracy: 0.9523\n",
            "Epoch 2/5\n",
            "938/938 [==============================] - 4s 4ms/step - loss: 0.0456 - accuracy: 0.9863\n",
            "Epoch 3/5\n",
            "938/938 [==============================] - 4s 4ms/step - loss: 0.0314 - accuracy: 0.9900\n",
            "Epoch 4/5\n",
            "938/938 [==============================] - 6s 6ms/step - loss: 0.0237 - accuracy: 0.9929\n",
            "Epoch 5/5\n",
            "938/938 [==============================] - 4s 4ms/step - loss: 0.0182 - accuracy: 0.9944\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fba601b0550>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images = train_images.reshape((60000, 28, 28, 1))\n",
        "train_images = train_images.astype(\"float32\") / 255\n",
        "test_images = test_images.reshape((10000, 28, 28, 1))\n",
        "test_images = test_images.astype(\"float32\") / 255\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "model.fit(train_images, train_labels, epochs=5, batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9mpKW-9oB4k"
      },
      "source": [
        "**Evaluating the convnet**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWzZ6D98oB4l",
        "outputId": "7f03ca22-e8b3-466f-86a6-c2af0ee201d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0220 - accuracy: 0.9929\n",
            "Test accuracy: 0.993\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUXCTQbooB4m"
      },
      "source": [
        "### The convolution operation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dense layers learn global patterns in their input feature space whereas\n",
        "convolution layers learn local patterns\n",
        "\n",
        "The patterns they learn are translation-invariant\n",
        "\n",
        "They can learn spatial hierarchies of patterns\n",
        "\n",
        "Convolution preserves the spatial relationship between pixels by learning image features using small squares (depending on the filter size) of input data\n",
        "\n",
        "Convolution: multiplying elementwise by filter and summing the multiplication\n",
        "outputs\n",
        "\n",
        "Ex) a 3x3 kernel or 3x3x1 filter acts on a 5x6 input image with stride 1 and outputs a 3x4 feature map.\n",
        "In fully connected sense, we need unshared 30x12 weights (input size x output size)"
      ],
      "metadata": {
        "id": "VTEhqITHxwW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### How convolution filter works?\n",
        "\n",
        "Different value of the filter matrix produce different feature maps for the same input image.\n",
        "\n",
        "CNN learns the values of filters during.\n",
        "\n",
        "The more filters the more features are extracted"
      ],
      "metadata": {
        "id": "r5SyKg9dyFUX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature map\n",
        "\n",
        "Size of a feature map is controlled by four parameters that we specify before training.\n",
        "\n",
        "\n",
        "*   filter size: size of each filter (height x width x |channel|)\n",
        "*   depth: number of filters (feature maps)\n",
        "*  stride: number of pixels by which we slide our filter matrix\n",
        "* zero-padding: padding the input matrix with zero around the border to make the output with the same size as the input. \n"
      ],
      "metadata": {
        "id": "XZL5kzjiydns"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ReLU (nonlinearity)\n",
        "\n",
        "ReLU for nonlinearity has been used after every convolution operation\n",
        "\n",
        "It is an elementwise operation (applied per pixel) and replaces all negative pixel values in the feature map by zero"
      ],
      "metadata": {
        "id": "yRhq8urgy-tK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqM6GVFqoB4m"
      },
      "source": [
        "#### Understanding border effects and padding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bljZLnihoB4n"
      },
      "source": [
        "#### Understanding convolution strides"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22ZmVdeNoB4n"
      },
      "source": [
        "### The max-pooling operation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Role of max pooling: to aggressively downsample feature maps\n",
        "\n",
        "Transformed via a hardcoded max tensoroperation"
      ],
      "metadata": {
        "id": "EK99oy4szMzL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need the features from the last\n",
        "convolution layer to contain\n",
        "information about the totality of the\n",
        "input\n",
        "\n",
        "The final feature map has 22 × 22 ×\n",
        "128 = 61,952 total coefficients per\n",
        "sample\n",
        "\n",
        "This is far too large for such a\n",
        "small model and would result in\n",
        "intense overfitting\n"
      ],
      "metadata": {
        "id": "HNKvjaR9zPUx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1BTAKPYoB4o"
      },
      "source": [
        "**An incorrectly structured convnet missing its max-pooling layers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8Dnj3AZmoB4o"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(28, 28, 1))\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
        "model_no_max_pool = keras.Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sD7AiuKoB4p"
      },
      "outputs": [],
      "source": [
        "model_no_max_pool.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9BF9eCQoB4q"
      },
      "source": [
        "## Training a convnet from scratch on a small dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpMNE0FkoB4q"
      },
      "source": [
        "### The relevance of deep learning for small-data problems"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading a Kaggle dataset in Google Colaboratory\n",
        "\n",
        "Access to the API is restricted to Kaggle users, you need to authenticate yourself.\n",
        "The kaggle package will look for your login credentials in a JSON file located at\n",
        "\n",
        "~/.kaggle/kaggle.json.\n",
        "\n",
        "First, you need to create a Kaggle API key and download it to your local machine\n",
        "\n",
        "Login -> My Account -> Account settings -> API section.\n",
        "\n",
        "Click the Create New API Token butten\n",
        "\n",
        "Second, go to your Colab notebook, and upload the API’s key JSON file to your\n",
        "Colab session by running the following code in a notebook cell:"
      ],
      "metadata": {
        "id": "nXGVgcFPzWCW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7ayCJ_GoB4r"
      },
      "source": [
        "### Downloading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "id": "bVKLvmzGoB4r",
        "outputId": "c0ab74d3-a06e-41eb-f626-8b2c28bc2222"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-409a11e4-22ac-4c93-8c86-711561ecfd98\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-409a11e4-22ac-4c93-8c86-711561ecfd98\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PwjO5EXoB4s"
      },
      "outputs": [],
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BIfWe4IoB4s"
      },
      "outputs": [],
      "source": [
        "!kaggle competitions download -c dogs-vs-cats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yh2qdoV8oB4t"
      },
      "outputs": [],
      "source": [
        "!unzip -qq train.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGg94fjboB4t"
      },
      "source": [
        "**Copying images to training, validation, and test directories**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWqyBbMnoB4u"
      },
      "outputs": [],
      "source": [
        "import os, shutil, pathlib\n",
        "\n",
        "original_dir = pathlib.Path(\"train\") #path to the directory where the original dataset was uncompressed\n",
        "new_base_dir = pathlib.Path(\"cats_vs_dogs_small\")  #directory where we will store our smaller dataset\n",
        "\n",
        "#utility f° to copy cat and dog images from index start_index to index end_index to the subdirectory new_base_dir/{subset_name}/cat{and/dog}\n",
        "#the subset_name will either train validation or test\n",
        "def make_subset(subset_name, start_index, end_index):\n",
        "    for category in (\"cat\", \"dog\"):\n",
        "        dir = new_base_dir / subset_name / category\n",
        "        os.makedirs(dir)\n",
        "        fnames = [f\"{category}.{i}.jpg\" for i in range(start_index, end_index)]\n",
        "        for fname in fnames:\n",
        "            shutil.copyfile(src=original_dir / fname,\n",
        "                            dst=dir / fname)\n",
        "\n",
        "make_subset(\"train\", start_index=0, end_index=1000)#create the training subset with the first 1000 images of each category\n",
        "make_subset(\"validation\", start_index=1000, end_index=1500) #create the validation subset with the next 500 images of each category\n",
        "make_subset(\"test\", start_index=1500, end_index=2500) #create the test subset with the next 1000 images of each category"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyelWXvzoB4u"
      },
      "source": [
        "### Building the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMqJyyM7oB4v"
      },
      "source": [
        "**Instantiating a small convnet for dogs vs. cats classification**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZU108KaoB4v"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "inputs = keras.Input(shape=(180, 180, 3)) #the model expects RGB images of size 180 x 180\n",
        "x = layers.Rescaling(1./255)(inputs) #rescale inputs to the [0,1] range by dividing them by 255\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iONLYpYVoB4v"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNbIXoSxoB4w"
      },
      "source": [
        "**Configuring the model for training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grvx4PnaoB4w"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"rmsprop\",\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hPfMbZgoB4w"
      },
      "source": [
        "### Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Read the picture files.\n",
        "2. Decode the JPEG content to RGB grids of pixels\n",
        "3. Convert these into floating-point tensors\n",
        "4. Resize them to a shared size (we’ll use 180 × 180)\n",
        "5. Pack them into batches (we’ll use batches of 32 images)"
      ],
      "metadata": {
        "id": "fU67Ssq4042v"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWJsck-moB4w"
      },
      "source": [
        "**Using `image_dataset_from_directory` to read images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKZOnsnooB4x"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "\n",
        "train_dataset = image_dataset_from_directory(\n",
        "    new_base_dir / \"train\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)\n",
        "validation_dataset = image_dataset_from_directory(\n",
        "    new_base_dir / \"validation\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)\n",
        "test_dataset = image_dataset_from_directory(\n",
        "    new_base_dir / \"test\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Understanding TF Dataset objects\n",
        "TensorFlow makes available the tf.data API to create efficient input pipelines\n",
        "\n",
        "The Dataset class handles many key features that would otherwise be\n",
        "cumbersome to implement yourself—in particular, asynchronous data prefetching\n",
        "\n",
        "The Dataset class also exposes a functional-style API for modifying datasets"
      ],
      "metadata": {
        "id": "R7-CNEfo1BF2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syLh6PatoB4x"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "random_numbers = np.random.normal(size=(1000, 16))\n",
        "dataset = tf.data.Dataset.from_tensor_slices(random_numbers) #the from_tensor_slices() class method can be used to create a Dataset from numpy array, or a tuple or dict of Numpy arrays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIclWy_JoB4y"
      },
      "outputs": [],
      "source": [
        "#Yielding single samples\n",
        "for i, element in enumerate(dataset):\n",
        "    print(element.shape)\n",
        "    if i >= 2:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKueYyL1oB4y"
      },
      "outputs": [],
      "source": [
        "#we can use the .batch() method to batch the data\n",
        "batched_dataset = dataset.batch(32)\n",
        "for i, element in enumerate(batched_dataset):\n",
        "    print(element.shape)\n",
        "    if i >= 2:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Range of useful dataset methods\n",
        ".shuffle(buffer_size): Shuffles elements within a buffer\n",
        "\n",
        ".prefetch(buffer_size): Prefetches a buffer of elements in GPU memory to achieve better device utilization.\n",
        "\n",
        ".map(callable): Applies an arbitrary transformation to each element of the dataset"
      ],
      "metadata": {
        "id": "-7eeN7N41iZP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQnC7rmuoB4z"
      },
      "outputs": [],
      "source": [
        "#mapping\n",
        "reshaped_dataset = dataset.map(lambda x: tf.reshape(x, (4, 4)))\n",
        "for i, element in enumerate(reshaped_dataset):\n",
        "    print(element.shape)\n",
        "    if i >= 2:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqRiDYVHoB4z"
      },
      "source": [
        "**Displaying the shapes of the data and labels yielded by the `Dataset`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyhPEymVoB4z"
      },
      "outputs": [],
      "source": [
        "for data_batch, labels_batch in train_dataset:\n",
        "    print(\"data batch shape:\", data_batch.shape)\n",
        "    print(\"labels batch shape:\", labels_batch.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOKPg45HoB40"
      },
      "source": [
        "**Fitting the model using a `Dataset`**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ModelCheckpoint callback to save the model after each epoch"
      ],
      "metadata": {
        "id": "4RymN4k815Hm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1-e7zAWoB40"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"convnet_from_scratch.keras\",\n",
        "        save_best_only=True,\n",
        "        monitor=\"val_loss\")\n",
        "]\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=30,\n",
        "    validation_data=validation_dataset,\n",
        "    callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9ONUmb7oB40"
      },
      "source": [
        "**Displaying curves of loss and accuracy during training**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the loss and accuracy of the model over the training and validation data"
      ],
      "metadata": {
        "id": "cn1duTyE19w5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u92BGa2boB41"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "accuracy = history.history[\"accuracy\"]\n",
        "val_accuracy = history.history[\"val_accuracy\"]\n",
        "loss = history.history[\"loss\"]\n",
        "val_loss = history.history[\"val_loss\"]\n",
        "epochs = range(1, len(accuracy) + 1)\n",
        "plt.plot(epochs, accuracy, \"bo\", label=\"Training accuracy\")\n",
        "plt.plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\n",
        "plt.title(\"Training and validation accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
        "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
        "plt.title(\"Training and validation loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’ll reload the model from its saved file to evaluate it as it was before it started overfitting."
      ],
      "metadata": {
        "id": "X50ASAke2Be7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7FJ2RGfoB41"
      },
      "source": [
        "**Evaluating the model on the test set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQieS9zEoB41"
      },
      "outputs": [],
      "source": [
        "test_model = keras.models.load_model(\"convnet_from_scratch.keras\")\n",
        "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because we have relatively few training samples (2,000), overfitting will be our\n",
        "number one concern.\n",
        "\n",
        "We learned dropout and weight decay (L2 regularization). We’re now going to\n",
        "learn data augmentation"
      ],
      "metadata": {
        "id": "BIcU6EiQ2F3J"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nOw7EceoB42"
      },
      "source": [
        "### Using data augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data augmentation takes the approach of generating more training data\n",
        "from existing training samples by augmenting the samples via a number of\n",
        "random transformations that yield believable-looking images\n",
        "\n",
        "In Keras, this can be done by adding a number of data augmentation layers at\n",
        "the start of your model.\n"
      ],
      "metadata": {
        "id": "-vOPtJ-M2LR1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFMQndTToB42"
      },
      "source": [
        "**Define a data augmentation stage to add to an image model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZ0eEyQZoB42"
      },
      "outputs": [],
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(0.1),\n",
        "        layers.RandomZoom(0.2),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RandomFlip(“horizontal”) is for randomly flipping half the images horizontally\n",
        "\n",
        "RandomRotation(0.1) Rotates the input images by a random value in the range [–10%, +10%]\n",
        "\n",
        "RandomZoom(0.2) Zooms in or out of the image by a random factor in the range [-20%, +20%]\n",
        "R"
      ],
      "metadata": {
        "id": "yS5JpaN82T8z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Using data augmentation"
      ],
      "metadata": {
        "id": "hDHawCTF2aJw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnOaZFWioB43"
      },
      "source": [
        "**Displaying some randomly augmented training images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcuqiYqkoB43"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "for images, _ in train_dataset.take(1): #we can use take(N) to only sample N batches from the dataset. This is equivalent to inserting a break in the loop after the Nth batch\n",
        "    for i in range(9):\n",
        "        augmented_images = data_augmentation(images) #apply the augmentation stage to the batch of images\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(augmented_images[0].numpy().astype(\"uint8\")) #display the first image in the output batch. For each of the 9 iterations this is a different augmentation of the same image\n",
        "        plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTQYTH4GoB43"
      },
      "source": [
        "**Defining a new convnet that includes image augmentation and dropout**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lc1FsxuWoB44"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(180, 180, 3))\n",
        "x = data_augmentation(inputs) ##!!!!!##\n",
        "x = layers.Rescaling(1./255)(x)\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dropout(0.5)(x) #!!!!#\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"rmsprop\",\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5OsbXXUoB44"
      },
      "source": [
        "**Training the regularized convnet**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHmi5lbioB44"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"convnet_from_scratch_with_augmentation.keras\",\n",
        "        save_best_only=True,\n",
        "        monitor=\"val_loss\")\n",
        "]\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=100,\n",
        "    validation_data=validation_dataset,\n",
        "    callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdMy0hxcoB45"
      },
      "source": [
        "**Evaluating the model on the test set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CLOUnbgoB45"
      },
      "outputs": [],
      "source": [
        "test_model = keras.models.load_model(\n",
        "    \"convnet_from_scratch_with_augmentation.keras\")\n",
        "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We get a test accuracy of 83.5%.\n",
        "It’s starting to look good!"
      ],
      "metadata": {
        "id": "x0PjmFuD4Z_m"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j07dZpvnoB46"
      },
      "source": [
        "## Leveraging a pretrained model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A common and highly effective approach to deep learning on small image datasets\n",
        "is to use a pretrained model\n",
        "\n",
        "Pretrained network is a saved network that was previously trained on a large\n",
        "dataset\n",
        "\n",
        "Motivations:\n",
        "Lots of data, time, resources needed to train and tune a neural network from\n",
        "scratch\n",
        "\n",
        "Cheaper, faster way of adapting a neural network by exploiting their\n",
        "generalization properties"
      ],
      "metadata": {
        "id": "BCjNpHc04fom"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCxXqljuoB46"
      },
      "source": [
        "### Feature extraction with a pretrained model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First part is called the\n",
        "convolutional base of the model.\n",
        "\n",
        "Convolutional base are likely to\n",
        "be more generic and more reusable\n",
        "\n",
        "Representations learned by the\n",
        "classifier will be specific to data\n",
        "on which the model was trained"
      ],
      "metadata": {
        "id": "ymeLV2Z84wQ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "List of image-classification models (all pretrained on the\n",
        "ImageNet dataset) that are available as part of keras:\n",
        "Xception, Inception V3, ResNet50, VGG16, VGG19, MobileNet\n",
        "More available from tensorflow hub:"
      ],
      "metadata": {
        "id": "dVYJGnb7432a"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kB7A6_vVoB46"
      },
      "source": [
        "**Instantiating the VGG16 convolutional base**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvsGVZxToB46"
      },
      "outputs": [],
      "source": [
        "conv_base = keras.applications.vgg16.VGG16(\n",
        "    weights=\"imagenet\",\n",
        "    include_top=False,\n",
        "    input_shape=(180, 180, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9kWZDvZoB47"
      },
      "outputs": [],
      "source": [
        "conv_base.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfPgHdWdoB47"
      },
      "source": [
        "#### Fast feature extraction without data augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’ll start by extracting features as NumPy arrays by calling the predict()\n",
        "method of the conv_base model on our training"
      ],
      "metadata": {
        "id": "K5nwb8Pq5sc2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BG7J7OSoB47"
      },
      "source": [
        "**Extracting the VGG16 features and corresponding labels**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QL2fI9ajoB48"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_features_and_labels(dataset):\n",
        "    all_features = []\n",
        "    all_labels = []\n",
        "    for images, labels in dataset:\n",
        "        preprocessed_images = keras.applications.vgg16.preprocess_input(images)\n",
        "        features = conv_base.predict(preprocessed_images)\n",
        "        all_features.append(features)\n",
        "        all_labels.append(labels)\n",
        "    return np.concatenate(all_features), np.concatenate(all_labels)\n",
        "\n",
        "train_features, train_labels =  get_features_and_labels(train_dataset)\n",
        "val_features, val_labels =  get_features_and_labels(validation_dataset)\n",
        "test_features, test_labels =  get_features_and_labels(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72unP58ioB48"
      },
      "outputs": [],
      "source": [
        "train_features.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training is very fast because we only have to deal with two Dense layers"
      ],
      "metadata": {
        "id": "oG28tVMX505n"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztNnlaMvoB48"
      },
      "source": [
        "**Defining and training the densely connected classifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyeDACXDoB48"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(5, 5, 512))\n",
        "x = layers.Flatten()(inputs) #note the use of the flatten layer before passing the features to a dense layer\n",
        "x = layers.Dense(256)(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"rmsprop\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "      filepath=\"feature_extraction.keras\",\n",
        "      save_best_only=True,\n",
        "      monitor=\"val_loss\")\n",
        "]\n",
        "history = model.fit(\n",
        "    train_features, train_labels,\n",
        "    epochs=20,\n",
        "    validation_data=(val_features, val_labels),\n",
        "    callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4PvwgrxoB49"
      },
      "source": [
        "**Plotting the results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16uYBm9doB49"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history[\"accuracy\"]\n",
        "val_acc = history.history[\"val_accuracy\"]\n",
        "loss = history.history[\"loss\"]\n",
        "val_loss = history.history[\"val_loss\"]\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.plot(epochs, acc, \"bo\", label=\"Training accuracy\")\n",
        "plt.plot(epochs, val_acc, \"b\", label=\"Validation accuracy\")\n",
        "plt.title(\"Training and validation accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
        "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
        "plt.title(\"Training and validation loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2B_hv_KoB4-"
      },
      "source": [
        "#### Feature extraction together with data augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a new model that chains together: 1) data augmentation, 2)\n",
        "freezing convolutional base, 3) a dense classifier"
      ],
      "metadata": {
        "id": "VyJH9Oud6Cmh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGEMFQJsoB4-"
      },
      "source": [
        "**Instantiating and freezing the VGG16 convolutional base**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5q7mBDESoB4-"
      },
      "outputs": [],
      "source": [
        "conv_base  = keras.applications.vgg16.VGG16(\n",
        "    weights=\"imagenet\",\n",
        "    include_top=False)\n",
        "conv_base.trainable = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9ThZYIDoB4-"
      },
      "source": [
        "**Printing the list of trainable weights before and after freezing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugumVTVSoB4_"
      },
      "outputs": [],
      "source": [
        "conv_base.trainable = True\n",
        "print(\"This is the number of trainable weights \"\n",
        "      \"before freezing the conv base:\", len(conv_base.trainable_weights))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glbWI8l0oB4_"
      },
      "outputs": [],
      "source": [
        "conv_base.trainable = False\n",
        "print(\"This is the number of trainable weights \"\n",
        "      \"after freezing the conv base:\", len(conv_base.trainable_weights))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mq14ZWQDoB4_"
      },
      "source": [
        "**Adding a data augmentation stage and a classifier to the convolutional base**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcGcinr7oB5A"
      },
      "outputs": [],
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(0.1),\n",
        "        layers.RandomZoom(0.2),\n",
        "    ]\n",
        ")\n",
        "\n",
        "inputs = keras.Input(shape=(180, 180, 3))\n",
        "x = data_augmentation(inputs) ##!!! apply data augmentation!!!\n",
        "x = keras.applications.vgg16.preprocess_input(x) #apply input value scaling\n",
        "x = conv_base(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(256)(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"rmsprop\",\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMnIwdWtoB5A"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"feature_extraction_with_data_augmentation.keras\",\n",
        "        save_best_only=True,\n",
        "        monitor=\"val_loss\")\n",
        "]\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=50,\n",
        "    validation_data=validation_dataset,\n",
        "    callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQ4MtRNRoB5A"
      },
      "source": [
        "**Evaluating the model on the test set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adp87ajboB5B"
      },
      "outputs": [],
      "source": [
        "test_model = keras.models.load_model(\n",
        "    \"feature_extraction_with_data_augmentation.keras\")\n",
        "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We get a test accuracy of 97.5%. This is only a modest improvement compared\n",
        "to the previous test accuracy"
      ],
      "metadata": {
        "id": "XH0H1i3L6xM_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9HgDYnzoB5B"
      },
      "source": [
        "### Fine-tuning a pretrained model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-tuning consists of unfreezing a few of the top layers of a frozen model base used for feature extraction, and jointly training both the newly added part of the model"
      ],
      "metadata": {
        "id": "dm3qPQPY60ma"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps:\n",
        "> Add your custom network on top of an already-trained base network\n",
        "\n",
        ">Freeze the base network\n",
        "\n",
        ">Train the part you added\n",
        "\n",
        ">Unfreeze some layers in the base network\n",
        "Jointly train both these layers and the part you added"
      ],
      "metadata": {
        "id": "hC4ic4RF64xw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ld-lzoqoB5B"
      },
      "outputs": [],
      "source": [
        "conv_base.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVdUZU9ioB5B"
      },
      "source": [
        "**Freezing all layers until the fourth from the last**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRUYxV4ooB5I"
      },
      "outputs": [],
      "source": [
        "conv_base.trainable = True\n",
        "for layer in conv_base.layers[:-4]:\n",
        "    layer.trainable = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSaSFB2toB5I"
      },
      "source": [
        "**Fine-tuning the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeXgwWUkoB5I"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=keras.optimizers.RMSprop(learning_rate=1e-5),\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"fine_tuning.keras\",\n",
        "        save_best_only=True,\n",
        "        monitor=\"val_loss\")\n",
        "]\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=30,\n",
        "    validation_data=validation_dataset,\n",
        "    callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGQYyhV1oB5J"
      },
      "outputs": [],
      "source": [
        "model = keras.models.load_model(\"fine_tuning.keras\")\n",
        "test_loss, test_acc = model.evaluate(test_dataset)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQdDZAgXoB5J"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convnets are the best type of machine-learning models for\n",
        "computer-vision\n",
        "\n",
        "On a small dataset, overfitting will be the main issue. Data augmentation is a powerful way\n",
        "\n",
        "It’s easy to reuse an existing convnet on a new dataset via transfer learning\n",
        "\n",
        "As a complement to feature extraction, you can use finetuning"
      ],
      "metadata": {
        "id": "6BVhnMqq7Enz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "x0C-1d067KL4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "chapter08_intro-to-dl-for-computer-vision.i",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}